# BERT-Topic-NLP

https://colab.research.google.com/drive/12YJ0eiu4nCiKX3KSxIU9SxJPdQFXHotq?usp=sharing 


# Objective:

The objective of this ICP is to comapare the performance of BERt topic and LDA topic selection.

# Approaches

At first, necessary libraries are imported. Sample data are extracted from Hugging face dataset and several transformer models are used  for sentence transformers and one dataset
is extracted from several Reddit topics using API and compared LDA performance with BERT sentence transformer.

# Datasets

For this ICP, several excerpt from several sources are used.

# Results:

Results are generated with several libraries and similarity between summarizer are evaluated.

LDA Coherence Score: 39.5%
Cross Encoder Score for LDA & BERT: 0.54, 0.55

                                                  

# Challenges

For this icp, some models didn't work for the extracted dataset.

# Planned Work

At first, BERTopic is installed and implemented and BERT tokenizer is implemented and dataset is classified. BERT performance is compared with various BERT transformer.
LDA is implemented and coherence score is generated.
